## 经典大语言模型算法:GPT、BERT

### GPT

- 什么是GPT？

即 Generative Pre-trained Transformer（生成式预训练Transformer 模型）

- 算法简单理解？

1. G（生成式）

大家可以把简单把 AI 本身理解为我们应该都很熟悉的一次函数，只不过拥有很多参数：

y = (w1 * x1 + w2 * x2 + w3 * x3 + ……) + b

x 可以看出我们输入给 AI 的内容，w 我们已经得到的参数，b 是一个偏置值。

2. P（预训练）

就是上面 AI「学习」得到 w1、w2……和 b，也就是总结一般规律的过程。

3. T（变换器）

Transformer 是一种神经网络结构，它利用了自注意力（self-attention）机制和多层编码器（encoder）与解码器（decoder）层，从而有效地处理长距离依赖关系和捕获不同层次的文本信息。

Transformer 解决的问题，就是 AI 如何快速准确地理解上下文，并且以通用且优雅、简洁的方式。而「注意力机制」就是解决这个问题的关键。

自注意力机制：自注意力是一种计算文本中不同位置之间关系的方法。它为文本中的每个词分配一个权重，以确定该词与其他词之间的关联程度。通过这种方式，模型可以了解上下文信息，以便在处理一词多义和上下文推理问题时作出合适的决策。

跨注意力机制：跨注意力是一种计算两个不同文本序列中各个位置之间关系的方法。它为一个序列中的每个词分配权重，以确定该词与另一个序列中的词之间的关联程度。通过这种方式，模型可以捕捉到两个序列之间的相互关系，以便在处理多模态数据、文本对齐和多任务学习等问题时作出正确的决策。

### BERT

- 什么是BERT？

即 Bidirectional Encoder Representations from Transformers（预训练语言表示模型）

- 算法简单理解？

1. 双向编码（Bidirectional Encoding）：

BERT是一个双向的语言表示模型，它能够同时考虑上下文信息，从而更好地理解句子的含义。
通过使用双向Transformer编码器，BERT能够在预测每个单词时同时考虑其左右上下文，从而更好地捕捉单词之间的关系。

2. 掩码语言模型（Masked Language Model，MLM）：

BERT的预训练任务之一是掩码语言模型，其中模型需要预测句子中被掩盖的单词。 通过这个任务，BERT学习到了单词之间的关联性，以及如何在上下文中填充缺失的单词。

3. 下一句预测（Next Sentence Prediction，NSP）：

BERT的另一个预训练任务是下一句预测，即判断两个句子是否是连续的。 这个任务有助于BERT学习句子之间的逻辑关系和连贯性，从而提高其在理解文本语境中的性能。

4. Fine-tuning（微调）：

与GPT类似，BERT在预训练之后通常需要在特定任务上进行微调，以适应具体的应用场景，如文本分类、命名实体识别等。

### Reference(参考文档)

* [GPT算法理解](https://36kr.com/p/2352006583589384)
